# -*- coding: utf-8 -*-
"""Day085_HW.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1-IwaMLtUKL-qAsjSYVSl2uxXtyS17V6h

## Work
1. 試改變 monitor "Validation Accuracy" 並比較結果
2. 調整 earlystop 的等待次數至 10, 25 並比較結果
"""

import os
import keras
from keras.layers import Dropout, BatchNormalization
from keras.callbacks import EarlyStopping
import matplotlib.pyplot as plt
# %matplotlib inline


# Disable GPU
os.environ["CUDA_VISIBLE_DEVICES"] = ""

(x_train, y_train), (x_test, y_test) = keras.datasets.cifar10.load_data()
print(f'x_train shape: {x_train.shape}\ny_train shape: {y_train.shape}')

## 資料前處理
def preproc_x(x, flatten=True):
    x = x / 255.
    if flatten:
        x = x.reshape(len(x), -1)
    return x

def preproc_y(y, n_classes=10):
    if y.shape[-1] == 1:
        y = keras.utils.to_categorical(y, n_classes)
    return y

# Preproc the inputs
x_train = preproc_x(x_train)
x_test = preproc_x(x_test)

# Preprc the outputs
y_train = preproc_y(y_train)
y_test = preproc_y(y_test)
print(f'x_train shape: {x_train.shape}\ny_train shape: {y_train.shape}')

# def build_mlp(input_data, output_data, n_neurons=[512, 256, 256, 256, 128], reg='l1'):
#     """
#     Build your own model
#     """
#     input_layer = keras.layers.Input([input_data.shape[-1]], name='input-layer')
#     for i, n_unit in enumerate(n_neurons):
#         if i == 0:
#             x = keras.layers.Dense(units=n_unit, activation='relu', name='hidden-layer'+str(i+1))(input_layer)
#             x = BatchNormalization()(x)
#         else:
#             x = keras.layers.Dense(units=n_unit, activation='relu', name='hidden-layer'+str(i+1))(x)
#             x = BatchNormalization()(x)
#     output_layer = keras.layers.Dense(units=output_data.shape[-1], activation='softmax', name='output-layer')(x)
#     model = keras.models.Model(inputs=input_layer, outputs=output_layer)
#     return model

def build_mlp(input_data, output_data, n_neurons=[512, 256, 256, 256, 128], bn=False):
    """
    Build your own model
    """
    input_layer = keras.layers.Input([input_data.shape[-1]], name='input-layer')

    for i, n_unit in enumerate(n_neurons):
        if i == 0:
            x = keras.layers.Dense(units=n_unit, activation='relu', name='hidden-layer'+str(i+1))(input_layer)
            if bn == True:
                x = BatchNormalization(name='BN-layer'+str(i+1))(x)
        else:
            x = keras.layers.Dense(units=n_unit, activation='relu', name='hidden-layer'+str(i+1))(x)
            if bn == True:
                x = BatchNormalization(name='BN-layer'+str(i+1))(x)

    output_layer = keras.layers.Dense(units=output_data.shape[-1], activation='softmax', name='output-layer')(x)
    model = keras.models.Model(outputs=output_layer, inputs=input_layer)
    return model

model = build_mlp(x_train, y_train, bn=True)
model.summary()

## 超參數設定
"""
Set your hyper-parameters
"""
BATCH_SIZE = 1024
EPOCHS = 50
PATIENCE = [5, 10]
MOMENTUM = 0.95
LEARNING_RATE = 0.003
BN = [False, True]

results = {}
for bn in BN:
    for patience in PATIENCE:
        keras.backend.clear_session()
        earlystop = EarlyStopping(monitor='val_acc', patience=patience)
        model = build_mlp(x_train, y_train, bn=bn)
        model.summary()
        print(f'bn: {bn}\npatience: {patience}')
        optimizer = keras.optimizers.SGD(lr=LEARNING_RATE, momentum=MOMENTUM, nesterov=True)
        model.compile(optimizer=optimizer, loss="categorical_crossentropy", metrics=["accuracy"])
        model.fit(x_train, y_train, 
                  batch_size=BATCH_SIZE, 
                  epochs=EPOCHS, 
                  callbacks=[earlystop], 
                  validation_data=(x_test, y_test), 
                  shuffle=True)

        #save data
        train_acc = model.history.history['acc']
        train_loss = model.history.history['loss']
        valid_acc = model.history.history['val_acc']
        valid_loss = model.history.history['val_loss']

        exp_tag = f'bn: {bn},patience: {patience}'
        results[exp_tag] = {'train-loss': train_loss,
                            'train-acc': train_acc,
                            'valid-loss': valid_loss,
                            'valid-acc': valid_acc}

color = ['r', 'b', 'g', 'm', 'k', 'y', 'c', 'g']
plt.figure(figsize=(10, 8))
for i, condition in enumerate(results.keys()):
    plt.plot(range(len(results[condition]['train-loss'])), results[condition]['train-loss'], '-', color=color[i], label=condition)
    plt.plot(range(len(results[condition]['valid-loss'])), results[condition]['valid-loss'], '--', color=color[i], label=condition)
plt.legend()
plt.title('Loss')
plt.show()

plt.figure(figsize=(10, 8))
for i, condition in enumerate(results.keys()):
    plt.plot(range(len(results[condition]['train-acc'])), results[condition]['train-acc'], '-', color=color[i], label=condition)
    plt.plot(range(len(results[condition]['valid-acc'])), results[condition]['valid-acc'], '--', color=color[i], label=condition)
plt.legend()
plt.title('Accuracy')
plt.show()