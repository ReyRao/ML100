# -*- coding: utf-8 -*-
"""Day078_HW.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1HDYbWUM3bP-TUkPi42UamMhgEuH_nqVt

## Work
1. 請嘗試將 preproc_x 替換成以每筆資料的 min/max 進行標準化至 -1 ~ 1 間，再進行訓練
2. 請嘗試將 mlp 疊更深 (e.g 5~10 層)，進行訓練後觀察 learning curve 的走勢
3. (optional) 請改用 GPU 進行訓練 (如果你有 GPU 的話)，比較使用 CPU 與 GPU 的訓練速度
"""

## 確認硬體資源
"""
Your code here
"""
#colab
!nvidia-smi

import os
import keras

# Try to enable GPU (optional)

(x_train, y_train), (x_test, y_test) = keras.datasets.cifar10.load_data()
print(f'x_train shape: {x_train.shape}\ny_train shape: {y_train.shape}')

## 資料前處理
"""
Your code here
"""
def preproc_x(x, flatten=True):
    x = x / 255
    if flatten == True:
        x = x.reshape(len(x), -1)
    return x

def preproc_y(y, n_classes=10):
    if y.shape[-1] == 1:
        y = keras.utils.to_categorical(y, n_classes)
    return y

# Preproc the inputs
x_train = preproc_x(x_train)
x_test = preproc_x(x_test)

# Preprc the outputs
y_train = preproc_y(y_train)
y_test = preproc_y(y_test)
print(f'x_train shape: {x_train.shape}\ny_train shape: {y_train.shape}')
print(x_train.shape[-1], x_train.shape[1:])

"""
Your code here
"""
def build_mlp(input_data, output_data, n_neurons=[512, 256, 256, 128]):
    input_layer = keras.layers.Input([input_data.shape[-1]], name='input_layer')
    
    for i, n_unit in enumerate(n_neurons):
        if i == 0:
            x = keras.layers.Dense(units=n_unit, activation='relu', name="hidden_layer"+str(i+1))(input_layer)
        
        else:
            x = keras.layers.Dense(units=n_unit, activation='relu', name="hidden_layer"+str(i+1))(x)
            
    output_layer = keras.layers.Dense(units=output_data.shape[-1], activation='softmax', name='output_layer')(x)
    model = keras.models.Model(inputs=input_layer, outputs=output_layer)
    
    return model

model = build_mlp(x_train, y_train)
model.summary()

## 超參數設定
LEARNING_RATE = 0.0001
EPOCHS = 100
BATCH_SIZE = 1024

optimizer = keras.optimizers.Adam(lr=LEARNING_RATE)
model.compile(loss="categorical_crossentropy", metrics=["accuracy"], optimizer=optimizer)

model.fit(x_train, y_train, 
          epochs=EPOCHS, 
          batch_size=BATCH_SIZE, 
          validation_data=(x_test, y_test), 
          shuffle=True)

import matplotlib.pyplot as plt


train_loss = model.history.history["loss"]
valid_loss = model.history.history["val_loss"]

train_acc = model.history.history["acc"]
valid_acc = model.history.history["val_acc"]

plt.plot(range(len(train_loss)), train_loss, label="train loss")
plt.plot(range(len(valid_loss)), valid_loss, label="valid loss")
plt.legend()
plt.title("Loss")
plt.show()

plt.plot(range(len(train_acc)), train_acc, label="train accuracy")
plt.plot(range(len(valid_acc)), valid_acc, label="valid accuracy")
plt.legend()
plt.title("Accuracy")
plt.show()